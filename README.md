# Data-Warehousing-Project

---
### CITS3401 - Data Warehousing (2020/SEM-1)
#### The University of Western Australia

---
The overall objectives of this project are to build a data warehouse from real-world datasets, and to carry out basic data mining activities including association rule mining, classification and clustering.

---
#### Datasets and Problem Domain
For the datasets and problem domain, you have one of the following three options:
- Prescribed datasets: For this project, we would like to recommend the US Adult Income dataset (i.e. adulttraining.csv) as the source of data for our data warehouse. 
  - A local copy of the necessary files (adult-training.csv.zip)
  
The project comprises of two sub-components, one on data warehouse design and implementation, the other on using the data for pattern discovery and predictive analytics.

---
#### Data Warehousing

Following four steps below of dimensional modelling (i.e. Kimball's four steps), design a data warehouse for the dataset(s) you have chosen.

1. Identify the process being modelled.
2. Determine the grain at which facts can be stored.
3. Choose the dimensions
4. Identify the numeric measures for the facts.

To realise the four steps, we can start by drawing and refining a StarNet with the above four questions in mind.

1. Think about a few business questions that your data warehouse could help answer.
2. Draw a StarNet with the aim to identify the dimensions and concept hierarchies for each dimension. This should be based on the lowest level information you have access to.
3. Use the StarNet footprints to illustrate how the business queries can be answered with your design. Refine the StarNet if the desired queries cannot be answered, for example, by adding more dimensions or concept hierarchies.
4. Once the StarNet diagram is completed, draw it using soware such as Microsoft Visio (free to download under the Azure Education Link: https://aka.ms/devtoolsforteaching) or a drawing program of your own choice. Paste it onto a Power BI Dashboard.
5. Implement a star or snowflake schema using SQL Server Management Studio (SSMS). Paste the database ER diagram generated by SSMS onto Power BI Dashboard.
6. Load the data from the csv files to populate the tables. You may need to create separate data files for your dimension tables.
7. Use SQL Server Data Tools to build a multi-dimensional analysis service solution, with a cube designed to answer your business queries. Make sure the concept hierarchies match your StarNet design. Paste the cube diagram to your Power BI Dashboard.
8. Use Power BI to visualise the data returned from your business queries.

---
#### Pattern Discovery and Predictive Analytics
One of the objectives of the data warehousing exercises in the first component is to produce clean, potentially aggregated, reduced or transformed data for pattern discovery and predicative analysis. In this second part of the project, we will demonstrate the typical data analytics processes using either Weka or other data analytic toolsets familiar to you (R or Python):
1. Association rule mining: select a subset of the attributes to mine interesting patterns. To rank the interestingness of the rules extracted, use support, confidence and li. Explain the top 5 rules (according to lift or confidence) that have the "income" range/bracket on the right-hand-side; explain the meaning of the rules in plain English. Given the rules, what recommendation will you give to a person who wants to improve income (e.g. a person should receive more education to earn more)? If you use other data sets, explain the top 5 rules you obtain in plain English and also provide recommendation accordingly.
2. Classification: use the "income" range/bracket as the target variable or choose your own target variable, build two decision tree models: 1) one uses a list of attributes selected based on your understanding, 2) the other uses attribute selection based on information gain. Visualise the decision trees, and explain in plain English how the generated trees can be interpreted. Compare the performance of the two models using 10 fold cross-validation, and explain the evaluation results.
3. Clustering: run a clustering algorithm of your choice and explain how the results can be interpreted.
4. Data reduction: perform numerosity reduction using sampling and feature reduction with PCA or DWT. Train a model on the reduced data and train another model on the original data. Compare the performance of the two models using 10 fold cross-validation, and explain the evaluation results.

---
THIS PROJECT WAS PART OF THE DATA WAREHOUSING UNIT (CITS3401) AT THE UNIVERSITY OF WESTERN AUSTRALIA.
